# References and Resources

## Official Documentation

### ROS 2
- [ROS 2 Documentation](https://docs.ros.org/en/humble/)
- [ROS 2 Tutorials](https://docs.ros.org/en/humble/Tutorials.html)
- [ROS 2 Design](https://design.ros2.org/)
- [ROS 2 Control](https://control.ros.org/)
- [Navigation2](https://navigation.ros.org/)

### Simulation
- [Gazebo Documentation](https://gazebosim.org/docs)
- [Unity Robotics Hub](https://github.com/Unity-Technologies/Unity-Robotics-Hub)
- [NVIDIA Isaac Sim](https://developer.nvidia.com/isaac-sim)
- [Isaac Gym](https://developer.nvidia.com/isaac-gym)

### NVIDIA Isaac
- [Isaac ROS](https://nvidia-isaac-ros.github.io/)
- [Isaac SDK](https://developer.nvidia.com/isaac-sdk)
- [NVIDIA Omniverse](https://developer.nvidia.com/omniverse)
- [TensorRT Documentation](https://docs.nvidia.com/deeplearning/tensorrt/)

### Deep Learning Frameworks
- [PyTorch Documentation](https://pytorch.org/docs/stable/index.html)
- [Hugging Face Transformers](https://huggingface.co/docs/transformers/)
- [ONNX Runtime](https://onnxruntime.ai/)
- [OpenCV Documentation](https://docs.opencv.org/)

## Research Papers

### Vision-Language-Action Models
- Brohan, A., et al. (2022). "RT-1: Robotics Transformer for Real-World Control at Scale." arXiv:2212.06817
- Brohan, A., et al. (2023). "RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control." arXiv:2307.15818
- Driess, D., et al. (2023). "PaLM-E: An Embodied Multimodal Language Model." arXiv:2303.03378
- Jiang, Y., et al. (2022). "VIMA: General Robot Manipulation with Multimodal Prompts." arXiv:2210.03094

### SLAM and Perception
- Mur-Artal, R., & Tard√≥s, J. D. (2017). "ORB-SLAM2: An Open-Source SLAM System for Monocular, Stereo, and RGB-D Cameras." IEEE Transactions on Robotics.
- Campos, C., et al. (2021). "ORB-SLAM3: An Accurate Open-Source Library for Visual, Visual-Inertial and Multi-Map SLAM." IEEE Transactions on Robotics.

### Vision-Language Understanding
- Radford, A., et al. (2021). "Learning Transferable Visual Models From Natural Language Supervision" (CLIP). ICML.
- Li, J., et al. (2022). "BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding." ICML.

### Humanoid Robotics
- Kajita, S., et al. (2003). "Biped Walking Pattern Generation by using Preview Control of Zero-Moment Point." ICRA.
- Wieber, P.-B. (2006). "Trajectory-Free Linear Model Predictive Control for Stable Walking." Humanoids.

### Reinforcement Learning for Robotics
- Schulman, J., et al. (2017). "Proximal Policy Optimization Algorithms." arXiv:1707.06347
- Haarnoja, T., et al. (2018). "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning." ICML.

## Books

### Robotics Fundamentals
- Craig, J. J. (2017). *Introduction to Robotics: Mechanics and Control* (4th ed.). Pearson.
- Siciliano, B., & Khatib, O. (Eds.). (2016). *Springer Handbook of Robotics* (2nd ed.). Springer.
- Lynch, K. M., & Park, F. C. (2017). *Modern Robotics: Mechanics, Planning, and Control*. Cambridge University Press.

### Computer Vision
- Szeliski, R. (2022). *Computer Vision: Algorithms and Applications* (2nd ed.). Springer.
- Hartley, R., & Zisserman, A. (2004). *Multiple View Geometry in Computer Vision* (2nd ed.). Cambridge University Press.

### Deep Learning
- Goodfellow, I., Bengio, Y., & Courville, A. (2016). *Deep Learning*. MIT Press.
- Zhang, A., et al. (2023). *Dive into Deep Learning*. Cambridge University Press.

## Online Courses and Tutorials

- [Robot Operating System (ROS) Specialization - Coursera](https://www.coursera.org/specializations/robotics)
- [Deep Learning Specialization - Coursera](https://www.coursera.org/specializations/deep-learning)
- [CS231n: Convolutional Neural Networks - Stanford](http://cs231n.stanford.edu/)
- [CS224n: Natural Language Processing - Stanford](http://web.stanford.edu/class/cs224n/)

## GitHub Repositories

### ROS 2 & Robotics
- [ros2/examples](https://github.com/ros2/examples) - Official ROS 2 examples
- [ros-planning/navigation2](https://github.com/ros-planning/navigation2) - Navigation2
- [ROBOTIS-GIT/ROBOTIS-OP3](https://github.com/ROBOTIS-GIT/ROBOTIS-OP3) - ROBOTIS OP3

### Vision-Language Models
- [openai/CLIP](https://github.com/openai/CLIP) - CLIP model
- [huggingface/transformers](https://github.com/huggingface/transformers) - Transformers library
- [google-research/robotics_transformer](https://github.com/google-research/robotics_transformer) - RT-1

### SLAM
- [UZ-SLAMLab/ORB_SLAM3](https://github.com/UZ-SLAMLab/ORB_SLAM3) - ORB-SLAM3
- [HKUST-Aerial-Robotics/VINS-Mono](https://github.com/HKUST-Aerial-Robotics/VINS-Mono) - VINS-Mono

## Community Resources

### Forums and Discussion
- [ROS Discourse](https://discourse.ros.org/) - ROS community forum
- [Robotics Stack Exchange](https://robotics.stackexchange.com/) - Q&A for robotics
- [r/robotics](https://www.reddit.com/r/robotics/) - Reddit robotics community

### Conferences
- **ICRA** - IEEE International Conference on Robotics and Automation
- **IROS** - IEEE/RSJ International Conference on Intelligent Robots and Systems
- **RSS** - Robotics: Science and Systems
- **CoRL** - Conference on Robot Learning

---

This book builds upon the collective knowledge and hard work of the robotics, AI, and open-source communities. We encourage you to explore these resources, contribute back to the community, and continue learning throughout your career in humanoid robotics and Physical AI.