# VLA Deployment for Humanoid Robots

## Overview

Deploying Vision-Language-Action (VLA) models on physical humanoid robots requires careful consideration of computational constraints, real-time performance, safety, and robustness. This chapter provides a comprehensive guide to deploying VLA systems in production environments.

## Deployment Architecture

### System Overview

```
┌────────────────────────────────────────────────────────────────┐
│                    VLA DEPLOYMENT SYSTEM                        │
│                                                                 │
│  ┌──────────────┐   ┌──────────────┐   ┌──────────────┐      │
│  │   SENSOR     │   │   VLA MODEL  │   │   ROBOT      │      │
│  │   INTERFACE  │──►│   INFERENCE  │──►│   CONTROL    │      │
│  │              │   │              │   │              │      │
│  │ • Cameras    │   │ • Vision Enc │   │ • Motion     │      │
│  │ • IMU        │   │ • Lang Enc   │   │   Planning   │      │
│  │ • Joints     │   │ • Fusion     │   │ • Safety     │      │
│  │ • Force      │   │ • Action Dec │   │ • Execution  │      │
│  └──────────────┘   └──────────────┘   └──────────────┘      │
│         │                   │                   │              │
│         │                   ▼                   │              │
│         │          ┌──────────────┐             │              │
│         │          │ OPTIMIZATION │             │              │
│         │          │              │             │              │
│         │          │ • TensorRT   │             │              │
│         │          │ • Quantize   │             │              │
│         │          │ • Pruning    │             │              │
│         │          └──────────────┘             │              │
│         │                                       │              │
│         └───────────────┬───────────────────────┘              │
│                         ▼                                       │
│                ┌──────────────┐                                │
│                │   SAFETY &   │                                │
│                │  MONITORING  │                                │
│                └──────────────┘                                │
└────────────────────────────────────────────────────────────────┘
```

## Model Optimization for Edge Deployment

### TensorRT Optimization

Optimize VLA models using NVIDIA TensorRT:

```python
import torch
import torch_tensorrt
from typing import List, Tuple

class VLATensorRTOptimizer:
    """
    Optimize VLA model for deployment using TensorRT.
    """

    def __init__(self, model_path: str):
        self.model = torch.jit.load(model_path)
        self.optimized_model = None

    def optimize_for_inference(
        self,
        input_shapes: dict,
        precision: str = "fp16",
        workspace_size: int = 2 << 30  # 2GB
    ):
        """
        Optimize model using TensorRT.

        Args:
            input_shapes: Dictionary of input shapes
            precision: "fp32", "fp16", or "int8"
            workspace_size: TensorRT workspace size in bytes
        """
        # Define input specifications
        inputs = []

        # Vision input
        if 'image' in input_shapes:
            inputs.append(
                torch_tensorrt.Input(
                    min_shape=input_shapes['image']['min'],
                    opt_shape=input_shapes['image']['opt'],
                    max_shape=input_shapes['image']['max'],
                    dtype=torch.float32
                )
            )

        # Proprioception input
        if 'proprio' in input_shapes:
            inputs.append(
                torch_tensorrt.Input(
                    min_shape=input_shapes['proprio']['min'],
                    opt_shape=input_shapes['proprio']['opt'],
                    max_shape=input_shapes['proprio']['max'],
                    dtype=torch.float32
                )
            )

        # Set precision
        enabled_precisions = {torch.float32}
        if precision == "fp16":
            enabled_precisions.add(torch.float16)
        elif precision == "int8":
            enabled_precisions.add(torch.int8)

        # Compile with TensorRT
        self.optimized_model = torch_tensorrt.compile(
            self.model,
            inputs=inputs,
            enabled_precisions=enabled_precisions,
            workspace_size=workspace_size,
            truncate_long_and_double=True,
            device=torch.device("cuda:0")
        )

        print(f"Model optimized with TensorRT using {precision} precision")

    def benchmark(self, sample_inputs: Tuple, num_iterations: int = 100):
        """Benchmark optimized model."""
        import time

        # Warmup
        for _ in range(10):
            with torch.no_grad():
                _ = self.optimized_model(*sample_inputs)

        # Benchmark
        torch.cuda.synchronize()
        start = time.time()

        for _ in range(num_iterations):
            with torch.no_grad():
                _ = self.optimized_model(*sample_inputs)

        torch.cuda.synchronize()
        end = time.time()

        avg_latency = (end - start) / num_iterations * 1000  # ms
        throughput = num_iterations / (end - start)  # samples/sec

        return {
            'avg_latency_ms': avg_latency,
            'throughput': throughput,
            'iterations': num_iterations
        }

    def save_optimized_model(self, save_path: str):
        """Save optimized TensorRT model."""
        if self.optimized_model is None:
            raise ValueError("Model not optimized yet")

        torch.jit.save(self.optimized_model, save_path)
        print(f"Optimized model saved to {save_path}")
```

### Quantization

Reduce model size and improve inference speed:

```python
import torch.quantization as quantization

class VLAQuantizer:
    """
    Quantize VLA model for efficient deployment.
    """

    def __init__(self, model):
        self.model = model
        self.quantized_model = None

    def quantize_dynamic(self):
        """
        Apply dynamic quantization (weights only).
        Best for models with large linear layers.
        """
        self.quantized_model = quantization.quantize_dynamic(
            self.model,
            {torch.nn.Linear, torch.nn.LSTM, torch.nn.GRU},
            dtype=torch.qint8
        )

        return self.quantized_model

    def quantize_static(self, calibration_loader):
        """
        Apply static quantization (weights and activations).
        Requires calibration data.
        """
        # Prepare model for quantization
        self.model.qconfig = quantization.get_default_qconfig('fbgemm')
        quantization.prepare(self.model, inplace=True)

        # Calibrate
        self.model.eval()
        with torch.no_grad():
            for batch in calibration_loader:
                self.model(batch['image'], batch['text'], batch['proprio'])

        # Convert to quantized model
        self.quantized_model = quantization.convert(self.model, inplace=False)

        return self.quantized_model

    def compare_performance(self, test_data):
        """Compare original and quantized model."""
        import time

        # Original model
        start = time.time()
        with torch.no_grad():
            orig_output = self.model(**test_data)
        orig_time = time.time() - start

        # Quantized model
        start = time.time()
        with torch.no_grad():
            quant_output = self.quantized_model(**test_data)
        quant_time = time.time() - start

        # Size comparison
        orig_size = sum(p.numel() * p.element_size() for p in self.model.parameters())
        quant_size = sum(p.numel() * p.element_size() for p in self.quantized_model.parameters())

        return {
            'speedup': orig_time / quant_time,
            'size_reduction': orig_size / quant_size,
            'orig_latency_ms': orig_time * 1000,
            'quant_latency_ms': quant_time * 1000,
            'orig_size_mb': orig_size / (1024 * 1024),
            'quant_size_mb': quant_size / (1024 * 1024)
        }
```

## ROS 2 Integration

### VLA ROS 2 Node

Create a ROS 2 node for VLA inference:

```python
import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Image, JointState, Imu
from std_msgs.msg import String, Float64MultiArray
from geometry_msgs.msg import Twist
from cv_bridge import CvBridge
import torch
import numpy as np

class VLANode(Node):
    """
    ROS 2 node for VLA model inference.
    """

    def __init__(self, model_path: str):
        super().__init__('vla_node')

        # Load optimized model
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model = torch.jit.load(model_path).to(self.device)
        self.model.eval()

        # CV Bridge
        self.bridge = CvBridge()

        # Declare parameters
        self.declare_parameter('inference_rate', 10.0)
        self.declare_parameter('action_scale', 1.0)
        self.declare_parameter('enable_safety_checks', True)

        # Get parameters
        self.inference_rate = self.get_parameter('inference_rate').value
        self.action_scale = self.get_parameter('action_scale').value
        self.enable_safety = self.get_parameter('enable_safety_checks').value

        # State
        self.current_image = None
        self.current_text = "idle"
        self.current_proprio = None
        self.last_action = None

        # Subscribers
        self.image_sub = self.create_subscription(
            Image, '/camera/rgb/image_raw', self.image_callback, 10)
        self.text_sub = self.create_subscription(
            String, '/vla/command', self.text_callback, 10)
        self.joint_sub = self.create_subscription(
            JointState, '/joint_states', self.joint_callback, 10)
        self.imu_sub = self.create_subscription(
            Imu, '/imu/data', self.imu_callback, 10)

        # Publishers
        self.action_pub = self.create_publisher(
            Float64MultiArray, '/vla/action', 10)
        self.status_pub = self.create_publisher(
            String, '/vla/status', 10)

        # Timer for inference
        self.inference_timer = self.create_timer(
            1.0 / self.inference_rate, self.inference_step)

        self.get_logger().info('VLA Node initialized')

    def image_callback(self, msg):
        """Update current image."""
        try:
            cv_image = self.bridge.imgmsg_to_cv2(msg, "rgb8")
            self.current_image = cv_image
        except Exception as e:
            self.get_logger().error(f'Image callback error: {e}')

    def text_callback(self, msg):
        """Update current command."""
        self.current_text = msg.data
        self.get_logger().info(f'Received command: {msg.data}')

    def joint_callback(self, msg):
        """Update proprioceptive state."""
        # Combine joint positions and velocities
        positions = np.array(msg.position)
        velocities = np.array(msg.velocity)

        if len(velocities) < len(positions):
            velocities = np.zeros_like(positions)

        self.current_proprio = np.concatenate([positions, velocities])

    def imu_callback(self, msg):
        """Process IMU data."""
        # Could incorporate IMU into proprioception
        pass

    def inference_step(self):
        """Execute VLA inference step."""
        if self.current_image is None or self.current_proprio is None:
            return

        try:
            # Prepare inputs
            image_tensor = self.preprocess_image(self.current_image)
            proprio_tensor = torch.FloatTensor(self.current_proprio).unsqueeze(0).to(self.device)

            # Inference
            with torch.no_grad():
                outputs = self.model(
                    image_tensor,
                    [self.current_text],
                    proprio_tensor
                )

            # Extract action
            action = outputs['actions'][0].cpu().numpy()

            # Scale action
            action = action * self.action_scale

            # Safety checks
            if self.enable_safety:
                action = self.apply_safety_checks(action)

            # Publish action
            action_msg = Float64MultiArray()
            action_msg.data = action.tolist()
            self.action_pub.publish(action_msg)

            self.last_action = action

            # Publish status
            status_msg = String()
            status_msg.data = f'Executing: {self.current_text}'
            self.status_pub.publish(status_msg)

        except Exception as e:
            self.get_logger().error(f'Inference error: {e}')

    def preprocess_image(self, image):
        """Preprocess image for model input."""
        import cv2
        from torchvision import transforms

        # Resize
        image = cv2.resize(image, (224, 224))

        # Convert to tensor and normalize
        transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize(
                mean=[0.485, 0.456, 0.406],
                std=[0.229, 0.224, 0.225]
            )
        ])

        image_tensor = transform(image).unsqueeze(0).to(self.device)
        return image_tensor

    def apply_safety_checks(self, action):
        """Apply safety constraints to action."""
        # Joint limits
        action = np.clip(action, -1.0, 1.0)

        # Velocity limits
        if self.last_action is not None:
            max_delta = 0.1  # Maximum change per step
            delta = action - self.last_action
            delta = np.clip(delta, -max_delta, max_delta)
            action = self.last_action + delta

        return action


def main(args=None):
    rclpy.init(args=args)

    vla_node = VLANode(model_path='/path/to/optimized_vla_model.pt')

    try:
        rclpy.spin(vla_node)
    except KeyboardInterrupt:
        vla_node.get_logger().info('VLA Node stopped')
    finally:
        vla_node.destroy_node()
        rclpy.shutdown()


if __name__ == '__main__':
    main()
```

## Safety Systems

### Multi-Layer Safety Architecture

Implement comprehensive safety checks:

```python
class VLASafetySystem:
    """
    Multi-layer safety system for VLA deployment.
    """

    def __init__(self):
        # Safety thresholds
        self.joint_limits = self.load_joint_limits()
        self.velocity_limits = {'max': 1.0, 'max_delta': 0.1}
        self.force_threshold = 50.0  # Newtons
        self.workspace_bounds = {
            'x': (-1.0, 1.0),
            'y': (-1.0, 1.0),
            'z': (0.0, 2.0)
        }

        # Safety state
        self.emergency_stop_active = False
        self.safety_violations = []

    def validate_action(
        self,
        action: np.ndarray,
        current_state: dict,
        predicted_outcome: dict
    ) -> dict:
        """
        Validate action before execution.

        Args:
            action: Proposed robot action
            current_state: Current robot state
            predicted_outcome: Predicted result of action

        Returns:
            Validation result with safe action
        """
        violations = []
        safe_action = action.copy()

        # Check joint limits
        if not self._check_joint_limits(action, current_state):
            violations.append('joint_limits')
            safe_action = self._clip_to_joint_limits(safe_action, current_state)

        # Check velocity limits
        if not self._check_velocity_limits(action, current_state):
            violations.append('velocity_limits')
            safe_action = self._limit_velocity(safe_action, current_state)

        # Check workspace bounds
        if not self._check_workspace(predicted_outcome):
            violations.append('workspace_bounds')
            safe_action = self._adjust_for_workspace(safe_action)

        # Check collision prediction
        if self._predict_collision(predicted_outcome):
            violations.append('collision_risk')
            safe_action = self._avoid_collision(safe_action, predicted_outcome)

        # Check stability
        if not self._check_stability(predicted_outcome):
            violations.append('stability_risk')
            safe_action = np.zeros_like(action)  # Stop if unstable

        return {
            'is_safe': len(violations) == 0,
            'violations': violations,
            'safe_action': safe_action,
            'original_action': action
        }

    def _check_joint_limits(self, action, state):
        """Check if action respects joint limits."""
        current_positions = state['joint_positions']
        future_positions = current_positions + action

        for i, (pos, limits) in enumerate(zip(future_positions, self.joint_limits)):
            if pos < limits['min'] or pos > limits['max']:
                return False
        return True

    def _check_velocity_limits(self, action, state):
        """Check velocity constraints."""
        last_action = state.get('last_action', np.zeros_like(action))
        delta = np.abs(action - last_action)
        return np.all(delta <= self.velocity_limits['max_delta'])

    def _check_workspace(self, outcome):
        """Check if end-effector stays in workspace."""
        ee_pos = outcome.get('end_effector_position', [0, 0, 0])

        for axis, bounds in self.workspace_bounds.items():
            idx = {'x': 0, 'y': 1, 'z': 2}[axis]
            if ee_pos[idx] < bounds[0] or ee_pos[idx] > bounds[1]:
                return False
        return True

    def _predict_collision(self, outcome):
        """Predict if action will cause collision."""
        # Simplified - would use actual collision checking
        min_clearance = outcome.get('min_clearance', float('inf'))
        return min_clearance < 0.05  # 5cm safety margin

    def _check_stability(self, outcome):
        """Check if action maintains robot stability."""
        com_position = outcome.get('center_of_mass', [0, 0, 0])
        support_polygon = outcome.get('support_polygon', None)

        if support_polygon is None:
            return True  # Can't verify

        # Check if CoM is within support polygon
        # Simplified 2D check
        return self._point_in_polygon(com_position[:2], support_polygon)

    def _point_in_polygon(self, point, polygon):
        """Check if point is inside polygon."""
        # Ray casting algorithm
        x, y = point
        n = len(polygon)
        inside = False

        p1x, p1y = polygon[0]
        for i in range(n + 1):
            p2x, p2y = polygon[i % n]
            if y > min(p1y, p2y):
                if y <= max(p1y, p2y):
                    if x <= max(p1x, p2x):
                        if p1y != p2y:
                            xinters = (y - p1y) * (p2x - p1x) / (p2y - p1y) + p1x
                        if p1x == p2x or x <= xinters:
                            inside = not inside
            p1x, p1y = p2x, p2y

        return inside

    def load_joint_limits(self):
        """Load joint limits from robot URDF."""
        # Simplified - would load from actual URDF
        return [
            {'min': -3.14, 'max': 3.14} for _ in range(12)
        ]
```

## Monitoring and Telemetry

### Performance Monitoring

Track system performance in production:

```python
import time
from collections import deque
import json

class VLAMonitor:
    """
    Monitor VLA system performance and health.
    """

    def __init__(self, window_size=100):
        self.metrics = {
            'inference_latency': deque(maxlen=window_size),
            'action_execution_time': deque(maxlen=window_size),
            'safety_violations': deque(maxlen=window_size),
            'success_rate': deque(maxlen=window_size),
            'command_processing_time': deque(maxlen=window_size)
        }

        self.start_time = time.time()
        self.total_commands = 0
        self.successful_commands = 0

    def log_inference(self, latency_ms):
        """Log inference latency."""
        self.metrics['inference_latency'].append(latency_ms)

    def log_execution(self, execution_time_ms, success):
        """Log action execution."""
        self.metrics['action_execution_time'].append(execution_time_ms)
        self.metrics['success_rate'].append(1.0 if success else 0.0)

        self.total_commands += 1
        if success:
            self.successful_commands += 1

    def log_safety_violation(self, violation_type):
        """Log safety violation."""
        self.metrics['safety_violations'].append(violation_type)

    def get_statistics(self):
        """Get current performance statistics."""
        import numpy as np

        stats = {}

        for metric_name, metric_data in self.metrics.items():
            if len(metric_data) > 0:
                if metric_name == 'safety_violations':
                    stats[metric_name] = {
                        'count': len(metric_data),
                        'types': list(set(metric_data))
                    }
                else:
                    stats[metric_name] = {
                        'mean': np.mean(metric_data),
                        'std': np.std(metric_data),
                        'min': np.min(metric_data),
                        'max': np.max(metric_data),
                        'p50': np.percentile(metric_data, 50),
                        'p95': np.percentile(metric_data, 95),
                        'p99': np.percentile(metric_data, 99)
                    }

        # Overall metrics
        stats['overall'] = {
            'uptime_seconds': time.time() - self.start_time,
            'total_commands': self.total_commands,
            'success_rate': self.successful_commands / max(self.total_commands, 1)
        }

        return stats

    def export_metrics(self, filepath):
        """Export metrics to file."""
        stats = self.get_statistics()
        with open(filepath, 'w') as f:
            json.dump(stats, f, indent=2)
```

## Best Practices for Deployment

1. **Model Optimization**: Use TensorRT and quantization for edge devices
2. **Safety First**: Implement multi-layer safety checks
3. **Real-time Guarantees**: Ensure inference meets latency requirements
4. **Monitoring**: Continuously track performance and health
5. **Graceful Degradation**: Handle failures without catastrophic consequences
6. **Versioning**: Track model and code versions for reproducibility
7. **Testing**: Extensive simulation and real-world testing before deployment
8. **Human Oversight**: Maintain ability for human intervention
9. **Logging**: Comprehensive logging for debugging and improvement
10. **Updates**: Support over-the-air model updates

Successful VLA deployment requires careful attention to optimization, safety, and monitoring to ensure reliable operation on physical humanoid robots in real-world environments.